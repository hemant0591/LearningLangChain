{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "305496a2-c561-4032-a590-8c2bf2b2fcfd",
   "metadata": {},
   "source": [
    "#### When the data might be stored in more than one vector store, we need to be able to route the query to the appropriate datasource to retrieve relevant docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61d5ef-eb6b-4e12-a1ff-f08217530565",
   "metadata": {},
   "source": [
    "### Logical Routing\n",
    "\n",
    "We give the LLM knowledge of the various data sources at our disposal and then let the LLM reason which data source to apply based on the userâ€™s query.\n",
    "In order to achieve this, we make use of function-calling models like GPT-3.5 Turbo to help classify each query into one of the available routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5c7ad0d-b4f5-46c2-a058-ad8e247409db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda, chain\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8725f6ae-f85f-4f33-b2e0-b6b0928baa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user's query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n",
    "        description=\"\"\"Given users question, choose which data source is most relevant.\"\"\",\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"You are an expert at routing a user question to the relevant data source.\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3719d10c-e525-4a83-b457-8afbe0af9d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RouteQuery(datasource='python_docs'), 'python_docs')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work: \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\" : question})\n",
    "result, result.datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccb8de-1442-470e-acad-9c33f0ab6ed3",
   "metadata": {},
   "source": [
    "Once we have the relevant datasource, we can route it to another function to execute additional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74f5ff6f-9b8f-4a83-9fbd-dad5a3500f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Routed to python docs'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def additiona_logic(result):\n",
    "    if result.datasource.lower() == \"python_docs\":\n",
    "        # logic here\n",
    "        return \"Routed to python docs\"\n",
    "    else:\n",
    "        return \"Routed to js docs\"\n",
    "\n",
    "# router_new = router | additiona_logic # works fine for simple functions (without async or strams)\n",
    "router_new = router | RunnableLambda(additiona_logic) # explicitly make it runnable\n",
    "router_new.invoke({\"question\" : question}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cce41f-ec97-4db7-b2cb-89f11599cb37",
   "metadata": {},
   "source": [
    "### Semantic Routing\n",
    "\n",
    "In semantic routing, we take multiple prompts that represent various data sources, embed them alongside user's query and perform vector similarity search to retrieve the most similar prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1778e5a4-04e3-4a59-a51e-168d549210ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts: prompt1 for physics questions, prompt2 for math questions\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. You are great at\n",
    " answering questions about physics in a concise and easy-to-understand manner.\n",
    " When you don't know the answer to a question, you admit that you don't know.\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering\n",
    " math questions. You are so good because you are able to break down hard\n",
    " problems into their component parts, answer the component parts, and then\n",
    " put them together to answer the broader question.\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf7f1b96-4bee-4f95-8a02-bdd15436038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c70e74f8-ee8c-41c3-a7fb-8e4dec79c731",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a306b606-c1f3-4a33-89bd-596f765f779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"What is a black hole\"\"\"\n",
    "\n",
    "query_embedding = embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2e644ca-d118-4223-b607-c2b08c3ce0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfdbc4af-012e-4dd7-b6cd-7b5a5d754e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17683697, 0.12066461])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86070f63-5abd-4542-9862-1f5cfb0056fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are a very smart physics professor. You are great at\\n answering questions about physics in a concise and easy-to-understand manner.\\n When you don't know the answer to a question, you admit that you don't know.\\nHere is a question:\\n{query}\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_prompt = prompt_templates[similarity.argmax()]\n",
    "similar_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21245e41-c48f-4c4e-8f8a-1ce6c988de35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template=\"You are a very smart physics professor. You are great at\\n answering questions about physics in a concise and easy-to-understand manner.\\n When you don't know the answer to a question, you admit that you don't know.\\nHere is a question:\\n{query}\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate.from_template(similar_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d762726-1558-4926-90e0-1aa40bd0f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def prompt_router(query):\n",
    "    query_embedding = embeddings.embed_query(query)\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    similar_prompt = prompt_templates[similarity.argmax()]\n",
    "    return PromptTemplate.from_template(similar_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9245c6-b032-45e4-8f28-7e7185b17964",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_router = (\n",
    "    prompt_router |\n",
    "    ChatOpenAI(model=\"gpt-3.5-turbo\") |\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "semantic_router.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55fafbdb-eade-4481-b900-6192382964bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Riemann Hypothesis is a famous unsolved problem in mathematics that was formulated by Bernhard Riemann in 1859. It deals with the distribution of prime numbers and states that all non-trivial zeros of the Riemann zeta function have a real part equal to 1/2. The Riemann zeta function is a complex function that is closely related to the distribution of prime numbers. \\n\\nThe Riemann Hypothesis is considered one of the most important unsolved problems in mathematics, and has important implications for number theory and the distribution of prime numbers. It is one of the seven Millennium Prize Problems identified by the Clay Mathematics Institute, with a reward of $1 million for its solution.\\n\\nWhile the Riemann Hypothesis has been extensively studied by mathematicians for over a century, no one has been able to prove it or disprove it. Many significant results in number theory have been obtained as a result of work on the Riemann Hypothesis, but it remains an open problem in mathematics.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What can you tell me about Riemann Hypothesis?\"\n",
    "\n",
    "semantic_router.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fa19b-4d21-435e-88bf-4fce50e783eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_env (langchain)",
   "language": "python",
   "name": "lc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
