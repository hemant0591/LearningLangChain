{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "zha3ieeNhGFa",
    "outputId": "81004702-605d-4207-a12f-63d2ffee2f09"
   },
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "# model = OpenAI(model=\"gpt-3.5-turbo\", max_tokens=1)\n",
    "# model.invoke(\"The sky is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ytSDglM0iuOG",
    "outputId": "cff770ed-1ea2-4700-f95a-749457069cb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", max_tokens=1)\n",
    "model.invoke(\"The sky is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5QVhK42i6LW",
    "outputId": "59a81825-e463-4540-a581-2f050166f2d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of South Sudan is Juba.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj1YJdSz9wjpQUjaf7b3w9qaJV24Z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ce29ccd6-f5c4-486c-bc4a-309f3b9b6df1-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI(max_completion_tokens=10)\n",
    "prompt = [HumanMessage(\"What is the capital of South Sudan?\")]\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-PeQMTIjKt9",
    "outputId": "c1449176-9278-4dd5-97e1-f0276d680e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maseru!!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_msg = SystemMessage(\n",
    " '''You are a helpful assistant that responds to questions with three\n",
    " exclamation marks.'''\n",
    ")\n",
    "\n",
    "human_msg = HumanMessage(\"What is the capital of Lesotho\")\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VfHnD-7zkg--"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the\n",
    "  context below. If the question cannot be answered using the information\n",
    "  provided, answer with \"I don't know\".\n",
    "  Context: {context}\n",
    "  Question: {question}\n",
    "  Answer: \"\"\")\n",
    "\n",
    "prompt = template.invoke({\n",
    "  \"context\": \"\"\"The most recent advancements in NLP are being driven by Large\n",
    "  Language Models (LLMs). These models outperform their smaller\n",
    "  counterparts and have become invaluable for developers who are creating\n",
    "  applications with NLP capabilities. Developers can tap into these\n",
    "  models through Hugging Face's `transformers` library, or by utilizing\n",
    "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
    "  libraries, respectively.\"\"\",\n",
    "  \"question\": \"Which model providers offer LLMs?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GmFx4ZF3nRQL",
    "outputId": "3bab4540-7478-4907-bad5-50a9eb232d1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face, OpenAI, and Cohere.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OpenAI(max_tokens=50)\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Thyis25rntSR",
    "outputId": "d939680f-04ff-42b5-8e36-ea1f43161c63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Answer the question based on the context below. If the\\n  question cannot be answered using the information provided, answer with\\n  \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large\\n  Language Models (LLMs). These models outperform their smaller\\n  counterparts and have become invaluable for developers who are creating\\n  applications with NLP capabilities. Developers can tap into these\\n  models through Hugging Face's `transformers` library, or by utilizing\\n  OpenAI and Cohere's offerings through the `openai` and `cohere`\\n  libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "  ('system', '''Answer the question based on the context below. If the\n",
    "  question cannot be answered using the information provided, answer with\n",
    "  \"I don\\'t know\".'''),\n",
    "  ('human', 'Context: {context}'),\n",
    "  ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "prompt = template.invoke({\n",
    "  \"context\": \"\"\"The most recent advancements in NLP are being driven by Large\n",
    "  Language Models (LLMs). These models outperform their smaller\n",
    "  counterparts and have become invaluable for developers who are creating\n",
    "  applications with NLP capabilities. Developers can tap into these\n",
    "  models through Hugging Face's `transformers` library, or by utilizing\n",
    "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
    "  libraries, respectively.\"\"\",\n",
    "  \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H20fyD-qpmOf",
    "outputId": "12e3c162-1d44-4b11-92a3-84b4c44c3a3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI and Cohere offer Large Language Models (LLMs) through their libraries, `openai` and `cohere`, respectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 152, 'total_tokens': 180, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj1YPUz009Cy4KD1IS3WF9ElubdfH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--1d36aecf-204a-4fb8-86a1-34ee17fd2ac0-0', usage_metadata={'input_tokens': 152, 'output_tokens': 28, 'total_tokens': 180, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI()\n",
    "model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OEQxwZuqi17"
   },
   "source": [
    "### JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_SKiFFuq8pi",
    "outputId": "19eeccf1-3c1d-4b61-cc88-64027b83ef0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemantgupta/LearningLangChain/lc_env/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:2041: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='They weigh the same', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "  '''An answer to the user's question along with justification for the\n",
    "  answer.'''\n",
    "  answer: str\n",
    "  '''The answer to the user's question'''\n",
    "  justification: str\n",
    "  '''Justification for the answer'''\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "result = structured_llm.invoke(\"\"\"What weights more, a pound of bricks or a pound of feathers\"\"\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dMWopL8JuUO0",
    "outputId": "c8622113-642e-4493-ced2-eb9e35be49f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They weigh the same'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "3W9-Vd_0uMba",
    "outputId": "6c9ab4d5-63f5-4666-cb6c-28c67a765171"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"answer\":\"They weigh the same\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.\"}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_result = result.model_dump_json()\n",
    "json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFkR32V7uq1F",
    "outputId": "7f009645-b64c-4bcc-f2f4-a687c92355fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'They weigh the same',\n",
       " 'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dict = result.model_dump()\n",
    "json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "KopNtqCIux_p",
    "outputId": "e78e7f58-7e15-4713-f5d6-6161f02afc2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dict['justification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bjURyEuvlBV"
   },
   "source": [
    "### CSV output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKfTAQRqu4Xq",
    "outputId": "874c798b-8c5d-4847-9e3d-7512f0ac3ddf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore', 'Dubai', 'Amsterdam']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "parser.invoke(\"Bangalore, Dubai, Amsterdam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LplsO4FwJtV"
   },
   "source": [
    "### Using runnable interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MWVBD9xyv3TP",
    "outputId": "6fc0220f-656b-4ba7-933c-6290728a6c84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 10, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj1YSkuwXi4gMstZW22ktNg3my4Xg', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--91229f51-e263-4614-be76-77d66f9ad594-0', usage_metadata={'input_tokens': 10, 'output_tokens': 9, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI()\n",
    "model.invoke(\"Hi There!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nVzS8naMwQkw",
    "outputId": "67d69987-caa3-4b71-9d3b-2d40ad231396"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "Goodbye! Have a great day!\n"
     ]
    }
   ],
   "source": [
    "completions = model.batch(['Hi There!', 'Bye'])\n",
    "for comp in completions:\n",
    "  print(comp.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeYpz-uIwdU6",
    "outputId": "ae6e596e-205b-446b-d8ad-5762343816a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Good\n",
      "bye\n",
      "!\n",
      " Have\n",
      " a\n",
      " great\n",
      " day\n",
      "!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream('Bye'):\n",
    "  print(token.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3SLjadD9w5EM",
    "outputId": "02bbd139-21d4-4a66-93a9-ff6657b6ef0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI and Cohere offer Large Language Models (LLMs) through their libraries, specifically the `openai` and `cohere` libraries.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 152, 'total_tokens': 182, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj1YUGsevEFmE4x5K1MTZWtDsJQpy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--c7be5cc2-8436-4ffb-8481-8ec57c37c319-0', usage_metadata={'input_tokens': 152, 'output_tokens': 30, 'total_tokens': 182, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "# the building blocks\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "  ('system', '''Answer the question based on the context below. If the\n",
    "  question cannot be answered using the information provided, answer with\n",
    "  \"I don\\'t know\".'''),\n",
    "  ('human', '''Context: The most recent advancements in NLP are being driven by Large\n",
    "  Language Models (LLMs). These models outperform their smaller\n",
    "  counterparts and have become invaluable for developers who are creating\n",
    "  applications with NLP capabilities. Developers can tap into these\n",
    "  models through Hugging Face's `transformers` library, or by utilizing\n",
    "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
    "  libraries, respectively.'''),\n",
    "  ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# combine them in a function\n",
    "# chain decorator adds the same Runnable interface for any function you write\n",
    "\n",
    "@chain\n",
    "def chatbot(value):\n",
    "  prompt = template.invoke(value)\n",
    "  return model.invoke(prompt)\n",
    "\n",
    "chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uL-xaE4_zwRo"
   },
   "outputs": [],
   "source": [
    "@chain\n",
    "def chatbot(value):\n",
    "  prompt = template.invoke(value)\n",
    "  for token in model.invoke(prompt):\n",
    "    yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvVNJopN19p-",
    "outputId": "5cdb7799-4663-44db-8d21-e0e41df44e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('content', 'OpenAI and Cohere offer Large Language Models (LLMs) through their libraries named `openai` and `cohere`, respectively.')\n",
      "('additional_kwargs', {'refusal': None})\n",
      "('response_metadata', {'token_usage': {'completion_tokens': 28, 'prompt_tokens': 152, 'total_tokens': 180, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj1YV45CkrPexSPkrghW338FTCwNR', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'lc_run--bc355bd9-2d03-46d4-b733-3ac8f3617ac0-0')\n",
      "('tool_calls', [])\n",
      "('invalid_tool_calls', [])\n",
      "('usage_metadata', {'input_tokens': 152, 'output_tokens': 28, 'total_tokens': 180, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
     ]
    }
   ],
   "source": [
    "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs?\"}):\n",
    "  print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0OivH7TR2GCr"
   },
   "outputs": [],
   "source": [
    "@chain\n",
    "async def chatbot(value):\n",
    "  prompt = await template.ainvoke(value)\n",
    "  return await model.ainvoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2aCdg0o3ys5",
    "outputId": "3fa4af52-3aee-4237-f93c-d84f8884a1ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hugging Face offers LLMs through its `transformers` library. OpenAI and Cohere also provide LLMs through their respective offerings in the `openai` and `cohere` libraries.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 152, 'total_tokens': 194, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj1YW87J5Z3KxsgIYrs3ovK28Z8Io', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--db59e473-d35a-495e-ad56-ecc7a248f170-0', usage_metadata={'input_tokens': 152, 'output_tokens': 42, 'total_tokens': 194, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-TECE3mi34Xv"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "questions = [\n",
    "    {\"question\": \"What is AI?\"},\n",
    "    {\"question\": \"What is AGI?\"},\n",
    "    {\"question\": \"What is LangChain?\"},\n",
    "]\n",
    "\n",
    "responses = await asyncio.gather(\n",
    "    *(chatbot.ainvoke(q) for q in questions)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLdEdH8T4sTc",
    "outputId": "773df6d8-d93e-412e-b1be-4db358d85fa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI stands for Artificial Intelligence. It refers to the simulation of human intelligence processes by machines, especially computer systems. In this context, Large Language Models (LLMs) powered by AI are driving advancements in NLP (Natural Language Processing) technologies.\n",
      "AGI stands for Artificial General Intelligence, which refers to the concept of a machine that is able to understand, learn, and apply knowledge in a wide range of tasks similar to human intelligence.\n",
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "for resp in responses:\n",
    "  print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8CLFI6jXlDy"
   },
   "source": [
    "### Declarative composition using LCEL (Langchain Expression Language)\n",
    "\n",
    "LCEL is a declarative language for composing LangChain components. LangChain\n",
    "compiles LCEL compositions to an optimized execution plan, with automatic paralleli‐\n",
    "zation, streaming, tracing, and async support.\n",
    "\n",
    "Let’s see the same example using LCEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vwFlT8e4zmm",
    "outputId": "e6cad3ea-a8d0-4943-f07f-c6809dc2741b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='OpenAI and Cohere offer Large Language Models (LLMs) through their libraries.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 152, 'total_tokens': 169, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cj1YYO924mUKSghlHvE2USr1FZJL6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9b1ec56c-1a48-4bcb-bab5-fa4e0d9916f9-0', usage_metadata={'input_tokens': 152, 'output_tokens': 17, 'total_tokens': 169, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate.from_messages([\n",
    "  ('system', '''Answer the question based on the context below. If the\n",
    "  question cannot be answered using the information provided, answer with\n",
    "  \"I don\\'t know\".'''),\n",
    "  ('human', '''Context: The most recent advancements in NLP are being driven by Large\n",
    "  Language Models (LLMs). These models outperform their smaller\n",
    "  counterparts and have become invaluable for developers who are creating\n",
    "  applications with NLP capabilities. Developers can tap into these\n",
    "  models through Hugging Face's `transformers` library, or by utilizing\n",
    "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
    "  libraries, respectively.'''),\n",
    "  ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "# combine them with the | operator\n",
    "chatbot = template | model\n",
    "\n",
    "chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHCJWYNkYh3s"
   },
   "source": [
    "In LangChain:\n",
    "\n",
    "template is a Runnable\n",
    "\n",
    "model is a Runnable\n",
    "\n",
    "template | model creates a new RunnableSequence\n",
    "\n",
    "Create a pipeline that looks like this:\n",
    "(input) → template.invoke → model.invoke → (output)\n",
    "\n",
    "It's syntactic sugar for building a chain, is similar to chatbot function we defined earlier with @chain.\n",
    "\n",
    "Use function when we have to do some custom modifications to the output of the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C29uwG5kYBsw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lc_env (langchain)",
   "language": "python",
   "name": "lc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
