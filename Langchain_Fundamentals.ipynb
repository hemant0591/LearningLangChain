{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICTHuBB2bEj4",
        "outputId": "869a6418-3264-4148-b97b-ca6f4c6e0106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-openai langchain-community\n",
        "!pip install -q langchain-text-splitters langchain-postgres"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "aeftidnCb1en"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.llms import OpenAI\n",
        "\n",
        "model = OpenAI(model=\"gpt-3.5-turbo\", max_tokens=1)\n",
        "model.invoke(\"The sky is\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "zha3ieeNhGFa",
        "outputId": "81004702-605d-4207-a12f-63d2ffee2f09"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3181504172.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The sky is\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         return (\n\u001b[0;32m--> 373\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    782\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    783\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 )\n\u001b[1;32m   1005\u001b[0m             ]\n\u001b[0;32m-> 1006\u001b[0;31m             return self._generate_helper(\n\u001b[0m\u001b[1;32m   1007\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             output = (\n\u001b[0;32m--> 810\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    811\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m                 )\n\u001b[1;32m    484\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                     \u001b[0;31m# V1 client returns the response in an PyDantic object instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnot_given\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     ) -> Completion | Stream[Completion]:\n\u001b[0;32m--> 541\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0;34m\"/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", max_tokens=1)\n",
        "model.invoke(\"The sky is\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ytSDglM0iuOG",
        "outputId": "cff770ed-1ea2-4700-f95a-749457069cb8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' pretty'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model = ChatOpenAI(max_completion_tokens=10)\n",
        "prompt = [HumanMessage(\"What is the capital of South Sudan?\")]\n",
        "model.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5QVhK42i6LW",
        "outputId": "59a81825-e463-4540-a581-2f050166f2d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The capital of South Sudan is Juba.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Cicg9zwC3Azp4An4a7UEyfN08By3z', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--03193af8-4a7e-4464-84a0-f18f8c304e87-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "system_msg = SystemMessage(\n",
        " '''You are a helpful assistant that responds to questions with three\n",
        " exclamation marks.'''\n",
        ")\n",
        "\n",
        "human_msg = HumanMessage(\"What is the capital of Lesotho\")\n",
        "response = model.invoke([system_msg, human_msg])\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-PeQMTIjKt9",
        "outputId": "c1449176-9278-4dd5-97e1-f0276d680e00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maseru!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = PromptTemplate.from_template(\"\"\"Answer the question based on the\n",
        "  context below. If the question cannot be answered using the information\n",
        "  provided, answer with \"I don't know\".\n",
        "  Context: {context}\n",
        "  Question: {question}\n",
        "  Answer: \"\"\")\n",
        "\n",
        "prompt = template.invoke({\n",
        "  \"context\": \"\"\"The most recent advancements in NLP are being driven by Large\n",
        "  Language Models (LLMs). These models outperform their smaller\n",
        "  counterparts and have become invaluable for developers who are creating\n",
        "  applications with NLP capabilities. Developers can tap into these\n",
        "  models through Hugging Face's `transformers` library, or by utilizing\n",
        "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
        "  libraries, respectively.\"\"\",\n",
        "  \"question\": \"Which model providers offer LLMs?\"\n",
        "})"
      ],
      "metadata": {
        "id": "VfHnD-7zkg--"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = OpenAI(max_tokens=50)\n",
        "model.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GmFx4ZF3nRQL",
        "outputId": "3bab4540-7478-4907-bad5-50a9eb232d1a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nHugging Face, OpenAI, and Cohere.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "  ('system', '''Answer the question based on the context below. If the\n",
        "  question cannot be answered using the information provided, answer with\n",
        "  \"I don\\'t know\".'''),\n",
        "  ('human', 'Context: {context}'),\n",
        "  ('human', 'Question: {question}'),\n",
        "])\n",
        "\n",
        "prompt = template.invoke({\n",
        "  \"context\": \"\"\"The most recent advancements in NLP are being driven by Large\n",
        "  Language Models (LLMs). These models outperform their smaller\n",
        "  counterparts and have become invaluable for developers who are creating\n",
        "  applications with NLP capabilities. Developers can tap into these\n",
        "  models through Hugging Face's `transformers` library, or by utilizing\n",
        "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
        "  libraries, respectively.\"\"\",\n",
        "  \"question\": \"Which model providers offer LLMs?\"\n",
        "})\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thyis25rntSR",
        "outputId": "d939680f-04ff-42b5-8e36-ea1f43161c63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Answer the question based on the context below. If the\\n  question cannot be answered using the information provided, answer with\\n  \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large\\n  Language Models (LLMs). These models outperform their smaller\\n  counterparts and have become invaluable for developers who are creating\\n  applications with NLP capabilities. Developers can tap into these\\n  models through Hugging Face's `transformers` library, or by utilizing\\n  OpenAI and Cohere's offerings through the `openai` and `cohere`\\n  libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI()\n",
        "model.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H20fyD-qpmOf",
        "outputId": "12e3c162-1d44-4b11-92a3-84b4c44c3a3e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hugging Face's `transformers` library, OpenAI, and Cohere offer Large Language Models (LLMs) for developers to utilize.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 152, 'total_tokens': 181, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CicgGOHBvD1NLaRtLBoah5x8n3Gbt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--9b9b5cd7-e9cd-4c81-9827-3cd5c14cfa24-0', usage_metadata={'input_tokens': 152, 'output_tokens': 29, 'total_tokens': 181, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON Output"
      ],
      "metadata": {
        "id": "3OEQxwZuqi17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class AnswerWithJustification(BaseModel):\n",
        "  '''An answer to the user's question along with justification for the\n",
        "  answer.'''\n",
        "  answer: str\n",
        "  '''The answer to the user's question'''\n",
        "  justification: str\n",
        "  '''Justification for the answer'''\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
        "result = structured_llm.invoke(\"\"\"What weights more, a pound of bricks or a pound of feathers\"\"\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_SKiFFuq8pi",
        "outputId": "19eeccf1-3c1d-4b61-cc88-64027b83ef0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py:2041: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AnswerWithJustification(answer='They weigh the same', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dMWopL8JuUO0",
        "outputId": "c8622113-642e-4493-ced2-eb9e35be49f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'They weigh the same'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_result = result.model_dump_json()\n",
        "json_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3W9-Vd_0uMba",
        "outputId": "6c9ab4d5-63f5-4666-cb6c-28c67a765171"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"answer\":\"They weigh the same\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_dict = result.model_dump()\n",
        "json_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFkR32V7uq1F",
        "outputId": "7f009645-b64c-4bcc-f2f4-a687c92355fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'They weigh the same',\n",
              " 'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_dict['justification']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KopNtqCIux_p",
        "outputId": "e78e7f58-7e15-4713-f5d6-6161f02afc2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the two substances is different.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV output"
      ],
      "metadata": {
        "id": "5bjURyEuvlBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "parser = CommaSeparatedListOutputParser()\n",
        "parser.invoke(\"Bangalore, Dubai, Amsterdam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKfTAQRqu4Xq",
        "outputId": "874c798b-8c5d-4847-9e3d-7512f0ac3ddf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Bangalore', 'Dubai', 'Amsterdam']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using runnable interface"
      ],
      "metadata": {
        "id": "_LplsO4FwJtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI()\n",
        "model.invoke(\"Hi There!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWVBD9xyv3TP",
        "outputId": "6fc0220f-656b-4ba7-933c-6290728a6c84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 10, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CicgM1Whh2SnBIiDZBsqUfwJWkmqb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--026952a4-2ca7-4991-bd95-c4953b5142b4-0', usage_metadata={'input_tokens': 10, 'output_tokens': 9, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completions = model.batch(['Hi There!', 'Bye'])\n",
        "for comp in completions:\n",
        "  print(comp.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVzS8naMwQkw",
        "outputId": "67d69987-caa3-4b71-9d3b-2d40ad231396"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n",
            "Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in model.stream('Bye'):\n",
        "  print(token.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeYpz-uIwdU6",
        "outputId": "ae6e596e-205b-446b-d8ad-5762343816a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Good\n",
            "bye\n",
            "!\n",
            " Let\n",
            " me\n",
            " know\n",
            " if\n",
            " you\n",
            " need\n",
            " any\n",
            " help\n",
            " in\n",
            " the\n",
            " future\n",
            ".\n",
            " Take\n",
            " care\n",
            "!\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import chain\n",
        "\n",
        "# the building blocks\n",
        "template = ChatPromptTemplate.from_messages([\n",
        "  ('system', '''Answer the question based on the context below. If the\n",
        "  question cannot be answered using the information provided, answer with\n",
        "  \"I don\\'t know\".'''),\n",
        "  ('human', '''Context: The most recent advancements in NLP are being driven by Large\n",
        "  Language Models (LLMs). These models outperform their smaller\n",
        "  counterparts and have become invaluable for developers who are creating\n",
        "  applications with NLP capabilities. Developers can tap into these\n",
        "  models through Hugging Face's `transformers` library, or by utilizing\n",
        "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
        "  libraries, respectively.'''),\n",
        "  ('human', 'Question: {question}'),\n",
        "])\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# combine them in a function\n",
        "# chain decorator adds the same Runnable interface for any function you write\n",
        "\n",
        "@chain\n",
        "def chatbot(value):\n",
        "  prompt = template.invoke(value)\n",
        "  return model.invoke(prompt)\n",
        "\n",
        "chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SLjadD9w5EM",
        "outputId": "02bbd139-21d4-4a66-93a9-ff6657b6ef0e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='OpenAI and Cohere offer Large Language Models (LLMs) through their libraries, `openai` and `cohere` respectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 152, 'total_tokens': 180, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CicgOcxHq3tL6UgP6Mg9S3CeGbSRL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--a65116a1-1bd2-4957-9c62-71dc87262cd8-0', usage_metadata={'input_tokens': 152, 'output_tokens': 28, 'total_tokens': 180, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "def chatbot(value):\n",
        "  prompt = template.invoke(value)\n",
        "  for token in model.invoke(prompt):\n",
        "    yield token"
      ],
      "metadata": {
        "id": "uL-xaE4_zwRo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs?\"}):\n",
        "  print(part)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvVNJopN19p-",
        "outputId": "5cdb7799-4663-44db-8d21-e0e41df44e4b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('content', 'OpenAI and Cohere offer Large Language Models (LLMs).')\n",
            "('additional_kwargs', {'refusal': None})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 13, 'prompt_tokens': 152, 'total_tokens': 165, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CicgPTg6elWRZJtdcbDtgkgI64Xeo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'lc_run--7104ba4d-2444-4170-9026-8387c6e054be-0')\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 152, 'output_tokens': 13, 'total_tokens': 165, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@chain\n",
        "async def chatbot(value):\n",
        "  prompt = await template.ainvoke(value)\n",
        "  return await model.ainvoke(prompt)"
      ],
      "metadata": {
        "id": "0OivH7TR2GCr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2aCdg0o3ys5",
        "outputId": "3fa4af52-3aee-4237-f93c-d84f8884a1ac"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='OpenAI and Cohere offer Large Language Models (LLMs) through their respective libraries, `openai` and `cohere`.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 152, 'total_tokens': 179, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CicgPJxf6Va9py8zwQ6cx4eet34VK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--0dd22ac6-9e11-475b-8473-d4963afe0bbc-0', usage_metadata={'input_tokens': 152, 'output_tokens': 27, 'total_tokens': 179, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "questions = [\n",
        "    {\"question\": \"What is AI?\"},\n",
        "    {\"question\": \"What is AGI?\"},\n",
        "    {\"question\": \"What is LangChain?\"},\n",
        "]\n",
        "\n",
        "responses = await asyncio.gather(\n",
        "    *(chatbot.ainvoke(q) for q in questions)\n",
        ")"
      ],
      "metadata": {
        "id": "-TECE3mi34Xv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for resp in responses:\n",
        "  print(resp.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLdEdH8T4sTc",
        "outputId": "773df6d8-d93e-412e-b1be-4db358d85fa0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI stands for artificial intelligence, which refers to the development of computer systems that can perform tasks that typically require human intelligence, such as speech recognition, decision-making, learning, and problem-solving.\n",
            "AGI stands for Artificial General Intelligence, which refers to a machine's ability to understand, learn, and apply knowledge across a wide range of tasks - similar to human intelligence. However, the context provided does not relate to AGI.\n",
            "I don't know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Declarative composition using LCEL (Langchain Expression Language)\n",
        "\n",
        "LCEL is a declarative language for composing LangChain components. LangChain\n",
        "compiles LCEL compositions to an optimized execution plan, with automatic paralleli‐\n",
        "zation, streaming, tracing, and async support.\n",
        "\n",
        "Let’s see the same example using LCEL:"
      ],
      "metadata": {
        "id": "Y8CLFI6jXlDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = ChatPromptTemplate.from_messages([\n",
        "  ('system', '''Answer the question based on the context below. If the\n",
        "  question cannot be answered using the information provided, answer with\n",
        "  \"I don\\'t know\".'''),\n",
        "  ('human', '''Context: The most recent advancements in NLP are being driven by Large\n",
        "  Language Models (LLMs). These models outperform their smaller\n",
        "  counterparts and have become invaluable for developers who are creating\n",
        "  applications with NLP capabilities. Developers can tap into these\n",
        "  models through Hugging Face's `transformers` library, or by utilizing\n",
        "  OpenAI and Cohere's offerings through the `openai` and `cohere`\n",
        "  libraries, respectively.'''),\n",
        "  ('human', 'Question: {question}'),\n",
        "])\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# combine them with the | operator\n",
        "chatbot = template | model\n",
        "\n",
        "chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vwFlT8e4zmm",
        "outputId": "e6cad3ea-a8d0-4943-f07f-c6809dc2741b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hugging Face, OpenAI, and Cohere offer Large Language Models (LLMs) that developers can access through their respective libraries `transformers`, `openai`, and `cohere`.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 152, 'total_tokens': 191, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CicisFCMaMV20LVyOg8A9CLcmL42i', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--31b02174-67de-44f3-bcb2-700f171b7633-0', usage_metadata={'input_tokens': 152, 'output_tokens': 39, 'total_tokens': 191, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LangChain:\n",
        "\n",
        "template is a Runnable\n",
        "\n",
        "model is a Runnable\n",
        "\n",
        "template | model creates a new RunnableSequence\n",
        "\n",
        "Create a pipeline that looks like this:\n",
        "(input) → template.invoke → model.invoke → (output)\n",
        "\n",
        "It's syntactic sugar for building a chain, is similar to chatbot function we defined earlier with @chain.\n",
        "\n",
        "Use function when we have to do some custom modifications to the output of the template."
      ],
      "metadata": {
        "id": "zHCJWYNkYh3s"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C29uwG5kYBsw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}